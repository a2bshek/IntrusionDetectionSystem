{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the KDD99 dataset\n",
    "dataset = pd.read_csv(\"../Preprocess/finalInt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "t_dataset = pd.read_csv(\"../Preprocess/finalInt_nohead.csv\")\n",
    "train_data = t_dataset[:int(len(dataset) * 0.9)]\n",
    "test_data = t_dataset[int(len(dataset) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(256, input_dim=100))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.Dense(1024))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.Dense(np.prod(train_data.shape[1:]), activation='tanh'))\n",
    "    model.add(tf.keras.layers.Reshape(target_shape=train_data.shape[1:]))\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(tf.keras.layers.Dense(1024))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Freeze the weights of the discriminator\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combined model for training the generator\n",
    "def make_gan(discriminator, generator):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "gan = make_gan(discriminator, generator)\n",
    "\n",
    "# Compile the combined model\n",
    "gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([436050, 218095, 456732,  42598, 154271, 300079, 290671,  13266,\\n             80490, 375894, 292359,  86011, 374649, 513205, 405037, 227073,\\n             97298,  84305, 189142, 122012, 500191, 294785, 165455, 451129,\\n            283835, 102021, 295544, 420603, 455014,  10095,   5959, 240437,\\n            161120, 236578, 478916, 198700, 301665, 241446, 466159, 396880,\\n            397556, 371807, 337898, 370141,  20150, 288715,  49852, 271938,\\n             58644, 236547, 249730,  22982, 349215, 508188, 337682, 226402,\\n            426308, 162477, 235079, 276414, 215339, 499232, 146885,   9273],\\n           dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[39m# Print progress\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m [D loss: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m, acc.: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m] [G loss: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, d_loss[\u001b[39m0\u001b[39m], \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m d_loss[\u001b[39m1\u001b[39m], g_loss))\n\u001b[1;32m---> 24\u001b[0m train(gan, discriminator, generator, train_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(gan, discriminator, generator, train_data, batch_size, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[39m# Train the discriminator\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, train_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], half_batch)\n\u001b[1;32m----> 7\u001b[0m     real_data \u001b[39m=\u001b[39m train_data[idx]\n\u001b[0;32m      8\u001b[0m     noise \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (half_batch, \u001b[39m100\u001b[39m))\n\u001b[0;32m      9\u001b[0m     fake_data \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39mpredict(noise)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3810\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3811\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3813\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6115\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6117\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:6173\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6171\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6172\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 6173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6175\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m   6176\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([436050, 218095, 456732,  42598, 154271, 300079, 290671,  13266,\\n             80490, 375894, 292359,  86011, 374649, 513205, 405037, 227073,\\n             97298,  84305, 189142, 122012, 500191, 294785, 165455, 451129,\\n            283835, 102021, 295544, 420603, 455014,  10095,   5959, 240437,\\n            161120, 236578, 478916, 198700, 301665, 241446, 466159, 396880,\\n            397556, 371807, 337898, 370141,  20150, 288715,  49852, 271938,\\n             58644, 236547, 249730,  22982, 349215, 508188, 337682, 226402,\\n            426308, 162477, 235079, 276414, 215339, 499232, 146885,   9273],\\n           dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Train the GAN\n",
    "def train(gan, discriminator, generator, train_data, batch_size=128, epochs=100):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    for epoch in range(epochs):\n",
    "        # Train the discriminator\n",
    "        idx = np.random.randint(0, train_data.shape[0], half_batch)\n",
    "        real_data = train_data[idx]\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        fake_data = generator.predict(noise)\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        fake_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, fake_labels)\n",
    "\n",
    "        # Print progress\n",
    "        print(\"Epoch: %d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch + 1, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "train(gan, discriminator, generator, train_data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new data\n",
    "noise = np.random.normal(0, 1, (100, 100))\n",
    "generated_data = generator.predict(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the generated data\n",
    "plt.scatter(dataset[:, 1], generated_data[:, 1], c=np.argmax(generated_data[:, 2:], axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the discriminator model\n",
    "# discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
    "# discriminator.save('../ML/GAN/discriminator_plot.h5')\n",
    "\n",
    "# # Save the generator model\n",
    "# generator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# generator.save('../ML/GAN/generator_plot.h5')\n",
    "\n",
    "# # Save the combined model\n",
    "# gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
    "# gan.save('../ML/GAN/gan_plot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the discriminator model\n",
    "# loaded_discriminator = tf.keras.models.load_model('../ML/GAN/discriminator.h5')\n",
    "\n",
    "# # Load the generator model\n",
    "# loaded_generator = tf.keras.models.load_model('../ML/GAN/generator.h5')\n",
    "\n",
    "# # Load the combined model\n",
    "# loaded_gan = tf.keras.models.load_model('../ML/GAN/gan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tensorflow Warning Fix\n",
    "# @tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
    "# def next_collatz(x):\n",
    "#     print(\"Tracing with\", x)\n",
    "#     return tf.where(x % 2 == 0, x // 2, 3 * x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom input data\n",
    "# input_data = np.array([[0,1,24,9,206,1491,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,17,20,0.0,0.0,0.0,0.0,1.0,0.0,0.1,0,255,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,11]])\n",
    "# # input_data = np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2]])\n",
    "\n",
    "# # Make a prediction using the model\n",
    "# prediction_data = discriminator.predict(input_data)\n",
    "\n",
    "# print(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  tcp  http  SF  215  45076  0.1  0.2  0.3  0.4  ...  0.17  0.00.6  \\\n",
      "0  0  tcp  http  SF  162   4528    0    0    0    0  ...     1     1.0   \n",
      "1  0  tcp  http  SF  236   1228    0    0    0    0  ...     2     1.0   \n",
      "2  0  tcp  http  SF  233   2032    0    0    0    0  ...     3     1.0   \n",
      "3  0  tcp  http  SF  239    486    0    0    0    0  ...     4     1.0   \n",
      "4  0  tcp  http  SF  238   1282    0    0    0    0  ...     5     1.0   \n",
      "\n",
      "   0.00.7  0.00.8  0.00.9  0.00.10  0.00.11  0.00.12  0.00.13  normal.  \n",
      "0     0.0    1.00     0.0      0.0      0.0      0.0      0.0  normal.  \n",
      "1     0.0    0.50     0.0      0.0      0.0      0.0      0.0  normal.  \n",
      "2     0.0    0.33     0.0      0.0      0.0      0.0      0.0  normal.  \n",
      "3     0.0    0.25     0.0      0.0      0.0      0.0      0.0  normal.  \n",
      "4     0.0    0.20     0.0      0.0      0.0      0.0      0.0  normal.  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KDD99 dataset from CSV file\n",
    "df = pd.read_csv('../dataset/kddcup.data.gz')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "727a28ee4c40c57140038d04f8464ce2838cdd9cbed412c77ad124b108b93625"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
